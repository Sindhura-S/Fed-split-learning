{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4lIj-3Cv1XQ",
        "outputId": "cfa3ef04-4c67-4243-8882-ae9b1777122f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (9.4.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.16.0+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->medmnist) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=2a1fbb49ccbfc96e09b4dc2e025426c46cb694b84eceafc04cc8907fc2a350eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, medmnist\n",
            "Successfully installed fire-0.5.0 medmnist-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVoL9PsP0oKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose the PneumoniaMNIST dataset\n",
        "data_flag = 'pneumoniamnist'\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = 2  # Binary classification for PneumoniaMNIST\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset with transforms\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "train_data = DataClass(split='train', download=True, transform=transform)\n",
        "test_data = DataClass(split='test', download=True, transform=transform)\n",
        "\n",
        "# Split data into 2 parts for clients\n",
        "split_size = len(train_data) // 2\n",
        "client_datasets = [Subset(train_data, range(i * split_size, (i + 1) * split_size)) for i in range(2)]\n",
        "\n",
        "# Create data loaders for each client\n",
        "batch_size = 32\n",
        "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Client Model\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 32, 1568)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hypernetwork\n",
        "class Hypernetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypernetwork, self).__init__()\n",
        "        self.fc = nn.Linear(2 * 1568, 3136)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Server Model\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(3136, 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# Initialize models and move to device\n",
        "clients = [ClientModel().to(device) for _ in range(2)]\n",
        "hypernetwork = Hypernetwork().to(device)\n",
        "server = ServerModel().to(device)\n",
        "\n",
        "# Define optimizers\n",
        "client_optimizers = [optim.SGD(client.parameters(), lr=0.01) for client in clients]\n",
        "hypernetwork_optimizer = optim.SGD(hypernetwork.parameters(), lr=0.01)\n",
        "server_optimizer = optim.SGD(server.parameters(), lr=0.01)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    server.train()\n",
        "    hypernetwork.train()\n",
        "    for client, optimizer in zip(clients, client_optimizers):\n",
        "        client.train()\n",
        "        for data, target in client_loaders[clients.index(client)]:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            client_output = client(data)\n",
        "\n",
        "            # Concatenate client outputs for the hypernetwork\n",
        "            aggregated_output = torch.cat([client_output for client in clients], dim=1)\n",
        "\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            loss = F.nll_loss(server_output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# def train(epoch):\n",
        "#     server.train()\n",
        "#     hypernetwork.train()\n",
        "#     for client, optimizer in zip(clients, client_optimizers):\n",
        "#         client.train()\n",
        "#         for data, target in client_loaders[clients.index(client)]:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             target = target.squeeze()\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             client_output = client(data)\n",
        "#             hypernetwork_output = hypernetwork(client_output)\n",
        "#             server_output = server(hypernetwork_output)\n",
        "#             loss = F.nll_loss(server_output, target)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    server.eval()\n",
        "    hypernetwork.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            client_outputs = [client(data) for client in clients]\n",
        "            aggregated_output = torch.cat(client_outputs, dim=1)\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            test_loss += F.nll_loss(server_output, target, reduction='sum').item()\n",
        "            pred = server_output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(1, 50):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBvtsvE8pf9H",
        "outputId": "3a9fdd22-5241-4f23-bbea-0b14206aeb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1 to /root/.medmnist/pneumoniamnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4170669/4170669 [00:01<00:00, 2956916.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist.npz\n",
            "Epoch 1: Loss: 0.6733518838882446\n",
            "Epoch 2: Loss: 0.6655266284942627\n",
            "Epoch 3: Loss: 0.6521260142326355\n",
            "Epoch 4: Loss: 0.42444413900375366\n",
            "Epoch 5: Loss: 0.41548973321914673\n",
            "Epoch 6: Loss: 0.5763504505157471\n",
            "Epoch 7: Loss: 0.32338136434555054\n",
            "Epoch 8: Loss: 0.4009271264076233\n",
            "Epoch 9: Loss: 0.5862756371498108\n",
            "Epoch 10: Loss: 0.39928188920021057\n",
            "Epoch 11: Loss: 0.5817214250564575\n",
            "Epoch 12: Loss: 0.4661250710487366\n",
            "Epoch 13: Loss: 0.3591834306716919\n",
            "Epoch 14: Loss: 0.33728569746017456\n",
            "Epoch 15: Loss: 0.3599308133125305\n",
            "Epoch 16: Loss: 0.5045976638793945\n",
            "Epoch 17: Loss: 0.5756531953811646\n",
            "Epoch 18: Loss: 0.33842259645462036\n",
            "Epoch 19: Loss: 0.46728765964508057\n",
            "Epoch 20: Loss: 0.5355247855186462\n",
            "Epoch 21: Loss: 0.5731016993522644\n",
            "Epoch 22: Loss: 0.3943222463130951\n",
            "Epoch 23: Loss: 0.540066659450531\n",
            "Epoch 24: Loss: 0.41308891773223877\n",
            "Epoch 25: Loss: 0.3283945322036743\n",
            "Epoch 26: Loss: 0.3636084794998169\n",
            "Epoch 27: Loss: 0.3454464077949524\n",
            "Epoch 28: Loss: 0.22444231808185577\n",
            "Epoch 29: Loss: 0.254972368478775\n",
            "Epoch 30: Loss: 0.1853681355714798\n",
            "Epoch 31: Loss: 0.4780057370662689\n",
            "Epoch 32: Loss: 0.3889743387699127\n",
            "Epoch 33: Loss: 0.26586082577705383\n",
            "Epoch 34: Loss: 0.1588207334280014\n",
            "Epoch 35: Loss: 0.2904396653175354\n",
            "Epoch 36: Loss: 0.19249671697616577\n",
            "Epoch 37: Loss: 0.1742190420627594\n",
            "Epoch 38: Loss: 0.13267068564891815\n",
            "Epoch 39: Loss: 0.29002806544303894\n",
            "Epoch 40: Loss: 0.11440230906009674\n",
            "Epoch 41: Loss: 0.31781482696533203\n",
            "Epoch 42: Loss: 0.10804405808448792\n",
            "Epoch 43: Loss: 0.20670002698898315\n",
            "Epoch 44: Loss: 0.07026690989732742\n",
            "Epoch 45: Loss: 0.17032597959041595\n",
            "Epoch 46: Loss: 0.07140274345874786\n",
            "Epoch 47: Loss: 0.1929055154323578\n",
            "Epoch 48: Loss: 0.09289061278104782\n",
            "Epoch 49: Loss: 0.16062434017658234\n",
            "\n",
            "Test set: Average loss: 0.4663, Accuracy: 528/624 (85%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nk5tk6M41HwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose the pneumoniamnist dataset\n",
        "data_flag = 'pneumoniamnist'\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = 2  # Binary classification for pneumoniamnist\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset with transforms\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "train_data = DataClass(split='train', download=True, transform=transform)\n",
        "test_data = DataClass(split='test', download=True, transform=transform)\n",
        "\n",
        "# Split data into 2 parts for clients\n",
        "split_size = len(train_data) // 2\n",
        "client_datasets = [Subset(train_data, range(i * split_size, (i + 1) * split_size)) for i in range(2)]\n",
        "\n",
        "# Create data loaders for each client\n",
        "batch_size = 32\n",
        "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Client Model\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 32, 1568)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hypernetwork\n",
        "class Hypernetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypernetwork, self).__init__()\n",
        "        self.fc = nn.Linear(2 * 1568, 6272)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Server Model\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(6272, 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# Initialize models and move to device\n",
        "clients = [ClientModel().to(device) for _ in range(2)]\n",
        "hypernetwork = Hypernetwork().to(device)\n",
        "server = ServerModel().to(device)\n",
        "\n",
        "# Define optimizers\n",
        "client_optimizers = [optim.SGD(client.parameters(), lr=0.01) for client in clients]\n",
        "hypernetwork_optimizer = optim.SGD(hypernetwork.parameters(), lr=0.01)\n",
        "server_optimizer = optim.SGD(server.parameters(), lr=0.01)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    server.train()\n",
        "    hypernetwork.train()\n",
        "    for client, optimizer in zip(clients, client_optimizers):\n",
        "        client.train()\n",
        "        for data, target in client_loaders[clients.index(client)]:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            client_output = client(data)\n",
        "\n",
        "            # Concatenate client outputs for the hypernetwork\n",
        "            aggregated_output = torch.cat([client_output for client in clients], dim=1)\n",
        "\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            loss = F.nll_loss(server_output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# def train(epoch):\n",
        "#     server.train()\n",
        "#     hypernetwork.train()\n",
        "#     for client, optimizer in zip(clients, client_optimizers):\n",
        "#         client.train()\n",
        "#         for data, target in client_loaders[clients.index(client)]:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             target = target.squeeze()\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             client_output = client(data)\n",
        "#             hypernetwork_output = hypernetwork(client_output)\n",
        "#             server_output = server(hypernetwork_output)\n",
        "#             loss = F.nll_loss(server_output, target)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    server.eval()\n",
        "    hypernetwork.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            client_outputs = [client(data) for client in clients]\n",
        "            aggregated_output = torch.cat(client_outputs, dim=1)\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            test_loss += F.nll_loss(server_output, target, reduction='sum').item()\n",
        "            pred = server_output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(1, 100):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05645988-9758-4064-db3e-b34ab2e92317",
        "id": "fSZnAxrRqrBj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist.npz\n",
            "Epoch 1: Loss: 0.6921656131744385\n",
            "Epoch 2: Loss: 0.6226744055747986\n",
            "Epoch 3: Loss: 0.49891021847724915\n",
            "Epoch 4: Loss: 0.5279533863067627\n",
            "Epoch 5: Loss: 0.41030046343803406\n",
            "Epoch 6: Loss: 0.5874865055084229\n",
            "Epoch 7: Loss: 0.540465235710144\n",
            "Epoch 8: Loss: 0.6240466237068176\n",
            "Epoch 9: Loss: 0.41853514313697815\n",
            "Epoch 10: Loss: 0.43741363286972046\n",
            "Epoch 11: Loss: 0.4619804620742798\n",
            "Epoch 12: Loss: 0.524450957775116\n",
            "Epoch 13: Loss: 0.5939149260520935\n",
            "Epoch 14: Loss: 0.44500336050987244\n",
            "Epoch 15: Loss: 0.38639166951179504\n",
            "Epoch 16: Loss: 0.45757707953453064\n",
            "Epoch 17: Loss: 0.5665512084960938\n",
            "Epoch 18: Loss: 0.40811577439308167\n",
            "Epoch 19: Loss: 0.42653530836105347\n",
            "Epoch 20: Loss: 0.292697548866272\n",
            "Epoch 21: Loss: 0.23548801243305206\n",
            "Epoch 22: Loss: 0.2106475830078125\n",
            "Epoch 23: Loss: 0.21919457614421844\n",
            "Epoch 24: Loss: 0.3307584822177887\n",
            "Epoch 25: Loss: 0.30621302127838135\n",
            "Epoch 26: Loss: 0.15884551405906677\n",
            "Epoch 27: Loss: 0.19729505479335785\n",
            "Epoch 28: Loss: 0.1824558824300766\n",
            "Epoch 29: Loss: 0.24769173562526703\n",
            "Epoch 30: Loss: 0.20375435054302216\n",
            "Epoch 31: Loss: 0.18857719004154205\n",
            "Epoch 32: Loss: 0.11659545451402664\n",
            "Epoch 33: Loss: 0.2688227891921997\n",
            "Epoch 34: Loss: 0.19354206323623657\n",
            "Epoch 35: Loss: 0.3827895224094391\n",
            "Epoch 36: Loss: 0.141184002161026\n",
            "Epoch 37: Loss: 0.09870463609695435\n",
            "Epoch 38: Loss: 0.3321685492992401\n",
            "Epoch 39: Loss: 0.13158713281154633\n",
            "Epoch 40: Loss: 0.11589332669973373\n",
            "Epoch 41: Loss: 0.08462346345186234\n",
            "Epoch 42: Loss: 0.08393669128417969\n",
            "Epoch 43: Loss: 0.2955647110939026\n",
            "Epoch 44: Loss: 0.054816316813230515\n",
            "Epoch 45: Loss: 0.13965632021427155\n",
            "Epoch 46: Loss: 0.07977578043937683\n",
            "Epoch 47: Loss: 0.11091666668653488\n",
            "Epoch 48: Loss: 0.2442830353975296\n",
            "Epoch 49: Loss: 0.028334634378552437\n",
            "Epoch 50: Loss: 0.0556066632270813\n",
            "Epoch 51: Loss: 0.17801804840564728\n",
            "Epoch 52: Loss: 0.08606703579425812\n",
            "Epoch 53: Loss: 0.09906671196222305\n",
            "Epoch 54: Loss: 0.08131559938192368\n",
            "Epoch 55: Loss: 0.146925687789917\n",
            "Epoch 56: Loss: 0.13878247141838074\n",
            "Epoch 57: Loss: 0.1496545672416687\n",
            "Epoch 58: Loss: 0.27407610416412354\n",
            "Epoch 59: Loss: 0.0751669704914093\n",
            "Epoch 60: Loss: 0.08934327214956284\n",
            "Epoch 61: Loss: 0.027730658650398254\n",
            "Epoch 62: Loss: 0.05055379867553711\n",
            "Epoch 63: Loss: 0.08076825737953186\n",
            "Epoch 64: Loss: 0.21009857952594757\n",
            "Epoch 65: Loss: 0.1708827167749405\n",
            "Epoch 66: Loss: 0.1322801560163498\n",
            "Epoch 67: Loss: 0.3287096321582794\n",
            "Epoch 68: Loss: 0.207408607006073\n",
            "Epoch 69: Loss: 0.09694460779428482\n",
            "Epoch 70: Loss: 0.11792727559804916\n",
            "Epoch 71: Loss: 0.3324594795703888\n",
            "Epoch 72: Loss: 0.1207323893904686\n",
            "Epoch 73: Loss: 0.1081247627735138\n",
            "Epoch 74: Loss: 0.19522544741630554\n",
            "Epoch 75: Loss: 0.07598192989826202\n",
            "Epoch 76: Loss: 0.026469118893146515\n",
            "Epoch 77: Loss: 0.29695215821266174\n",
            "Epoch 78: Loss: 0.05065546929836273\n",
            "Epoch 79: Loss: 0.020295893773436546\n",
            "Epoch 80: Loss: 0.023921556770801544\n",
            "Epoch 81: Loss: 0.1942739188671112\n",
            "Epoch 82: Loss: 0.10831253975629807\n",
            "Epoch 83: Loss: 0.016689715906977654\n",
            "Epoch 84: Loss: 0.12374997138977051\n",
            "Epoch 85: Loss: 0.15275806188583374\n",
            "Epoch 86: Loss: 0.053895726799964905\n",
            "Epoch 87: Loss: 0.12092416733503342\n",
            "Epoch 88: Loss: 0.09008864313364029\n",
            "Epoch 89: Loss: 0.2262907773256302\n",
            "Epoch 90: Loss: 0.0427480973303318\n",
            "Epoch 91: Loss: 0.20525699853897095\n",
            "Epoch 92: Loss: 0.43280458450317383\n",
            "Epoch 93: Loss: 0.1795610785484314\n",
            "Epoch 94: Loss: 0.026331866160035133\n",
            "Epoch 95: Loss: 0.1419162154197693\n",
            "Epoch 96: Loss: 0.07072644680738449\n",
            "Epoch 97: Loss: 0.07086484134197235\n",
            "Epoch 98: Loss: 0.09074994921684265\n",
            "Epoch 99: Loss: 0.09663160145282745\n",
            "\n",
            "Test set: Average loss: 0.5577, Accuracy: 526/624 (84%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkdRtJ7K1rRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose the pneumoniamnist dataset\n",
        "data_flag = 'pneumoniamnist'\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = 2  # Binary classification for pneumoniamnist\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset with transforms\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "train_data = DataClass(split='train', download=True, transform=transform)\n",
        "test_data = DataClass(split='test', download=True, transform=transform)\n",
        "\n",
        "# Split data into 2 parts for clients\n",
        "split_size = len(train_data) // 2\n",
        "client_datasets = [Subset(train_data, range(i * split_size, (i + 1) * split_size)) for i in range(2)]\n",
        "\n",
        "# Create data loaders for each client\n",
        "batch_size = 32\n",
        "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Client Model\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 32, 1568)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hypernetwork\n",
        "class Hypernetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypernetwork, self).__init__()\n",
        "        self.fc = nn.Linear(2 * 1568, 9408)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Server Model\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(9408,128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# Initialize models and move to device\n",
        "clients = [ClientModel().to(device) for _ in range(2)]\n",
        "hypernetwork = Hypernetwork().to(device)\n",
        "server = ServerModel().to(device)\n",
        "\n",
        "# Define optimizers\n",
        "client_optimizers = [optim.SGD(client.parameters(), lr=0.01) for client in clients]\n",
        "hypernetwork_optimizer = optim.SGD(hypernetwork.parameters(), lr=0.01)\n",
        "server_optimizer = optim.SGD(server.parameters(), lr=0.01)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    server.train()\n",
        "    hypernetwork.train()\n",
        "    for client, optimizer in zip(clients, client_optimizers):\n",
        "        client.train()\n",
        "        for data, target in client_loaders[clients.index(client)]:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            client_output = client(data)\n",
        "\n",
        "            # Concatenate client outputs for the hypernetwork\n",
        "            aggregated_output = torch.cat([client_output for client in clients], dim=1)\n",
        "\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            loss = F.nll_loss(server_output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# def train(epoch):\n",
        "#     server.train()\n",
        "#     hypernetwork.train()\n",
        "#     for client, optimizer in zip(clients, client_optimizers):\n",
        "#         client.train()\n",
        "#         for data, target in client_loaders[clients.index(client)]:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             target = target.squeeze()\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             client_output = client(data)\n",
        "#             hypernetwork_output = hypernetwork(client_output)\n",
        "#             server_output = server(hypernetwork_output)\n",
        "#             loss = F.nll_loss(server_output, target)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    server.eval()\n",
        "    hypernetwork.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            client_outputs = [client(data) for client in clients]\n",
        "            aggregated_output = torch.cat(client_outputs, dim=1)\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            test_loss += F.nll_loss(server_output, target, reduction='sum').item()\n",
        "            pred = server_output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(1, 100):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e163cfab-a361-4603-b84f-5c779896556c",
        "id": "0hGQdoQFsGjY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist.npz\n",
            "Epoch 1: Loss: 0.680432140827179\n",
            "Epoch 2: Loss: 0.6306365728378296\n",
            "Epoch 3: Loss: 0.5416367053985596\n",
            "Epoch 4: Loss: 0.7692310810089111\n",
            "Epoch 5: Loss: 0.5918901562690735\n",
            "Epoch 6: Loss: 0.6436994671821594\n",
            "Epoch 7: Loss: 0.41212257742881775\n",
            "Epoch 8: Loss: 0.47893026471138\n",
            "Epoch 9: Loss: 0.4861050844192505\n",
            "Epoch 10: Loss: 0.40638914704322815\n",
            "Epoch 11: Loss: 0.7082064747810364\n",
            "Epoch 12: Loss: 0.49489593505859375\n",
            "Epoch 13: Loss: 0.5097054243087769\n",
            "Epoch 14: Loss: 0.5312706828117371\n",
            "Epoch 15: Loss: 0.5541714429855347\n",
            "Epoch 16: Loss: 0.5407376289367676\n",
            "Epoch 17: Loss: 0.6039213538169861\n",
            "Epoch 18: Loss: 0.49199825525283813\n",
            "Epoch 19: Loss: 0.43319809436798096\n",
            "Epoch 20: Loss: 0.48157745599746704\n",
            "Epoch 21: Loss: 0.5377501845359802\n",
            "Epoch 22: Loss: 0.2899913787841797\n",
            "Epoch 23: Loss: 0.34463822841644287\n",
            "Epoch 24: Loss: 0.3387990891933441\n",
            "Epoch 25: Loss: 0.2910977900028229\n",
            "Epoch 26: Loss: 0.2790260314941406\n",
            "Epoch 27: Loss: 0.29247307777404785\n",
            "Epoch 28: Loss: 0.22352777421474457\n",
            "Epoch 29: Loss: 0.40122008323669434\n",
            "Epoch 30: Loss: 0.27138030529022217\n",
            "Epoch 31: Loss: 0.11068800836801529\n",
            "Epoch 32: Loss: 0.30344992876052856\n",
            "Epoch 33: Loss: 0.24683642387390137\n",
            "Epoch 34: Loss: 0.15597441792488098\n",
            "Epoch 35: Loss: 0.24025967717170715\n",
            "Epoch 36: Loss: 0.2330119013786316\n",
            "Epoch 37: Loss: 0.10694026947021484\n",
            "Epoch 38: Loss: 0.25981542468070984\n",
            "Epoch 39: Loss: 0.05833224952220917\n",
            "Epoch 40: Loss: 0.25590625405311584\n",
            "Epoch 41: Loss: 0.1448061168193817\n",
            "Epoch 42: Loss: 0.12507182359695435\n",
            "Epoch 43: Loss: 0.09376673400402069\n",
            "Epoch 44: Loss: 0.08933041244745255\n",
            "Epoch 45: Loss: 0.18735161423683167\n",
            "Epoch 46: Loss: 0.2822674512863159\n",
            "Epoch 47: Loss: 0.14806708693504333\n",
            "Epoch 48: Loss: 0.16126695275306702\n",
            "Epoch 49: Loss: 0.19295014441013336\n",
            "Epoch 50: Loss: 0.19495587050914764\n",
            "Epoch 51: Loss: 0.2785237431526184\n",
            "Epoch 52: Loss: 0.030208395794034004\n",
            "Epoch 53: Loss: 0.07340599596500397\n",
            "Epoch 54: Loss: 0.13489171862602234\n",
            "Epoch 55: Loss: 0.04888680577278137\n",
            "Epoch 56: Loss: 0.1985470950603485\n",
            "Epoch 57: Loss: 0.06240970641374588\n",
            "Epoch 58: Loss: 0.15614961087703705\n",
            "Epoch 59: Loss: 0.1257806122303009\n",
            "Epoch 60: Loss: 0.15632005035877228\n",
            "Epoch 61: Loss: 0.08429490029811859\n",
            "Epoch 62: Loss: 0.1673182100057602\n",
            "Epoch 63: Loss: 0.23119179904460907\n",
            "Epoch 64: Loss: 0.07989128679037094\n",
            "Epoch 65: Loss: 0.05992215871810913\n",
            "Epoch 66: Loss: 0.12767936289310455\n",
            "Epoch 67: Loss: 0.15030425786972046\n",
            "Epoch 68: Loss: 0.09152433276176453\n",
            "Epoch 69: Loss: 0.38677793741226196\n",
            "Epoch 70: Loss: 0.22167396545410156\n",
            "Epoch 71: Loss: 0.4216124415397644\n",
            "Epoch 72: Loss: 0.11210475862026215\n",
            "Epoch 73: Loss: 0.046174582093954086\n",
            "Epoch 74: Loss: 0.1573040634393692\n",
            "Epoch 75: Loss: 0.08311311155557632\n",
            "Epoch 76: Loss: 0.05883857235312462\n",
            "Epoch 77: Loss: 0.09258176386356354\n",
            "Epoch 78: Loss: 0.037014394998550415\n",
            "Epoch 79: Loss: 0.11708002537488937\n",
            "Epoch 80: Loss: 0.1880485713481903\n",
            "Epoch 81: Loss: 0.13751207292079926\n",
            "Epoch 82: Loss: 0.14001882076263428\n",
            "Epoch 83: Loss: 0.04325953871011734\n",
            "Epoch 84: Loss: 0.1038954108953476\n",
            "Epoch 85: Loss: 0.35071510076522827\n",
            "Epoch 86: Loss: 0.1507008820772171\n",
            "Epoch 87: Loss: 0.04807015508413315\n",
            "Epoch 88: Loss: 0.061595890671014786\n",
            "Epoch 89: Loss: 0.04423219710588455\n",
            "Epoch 90: Loss: 0.052780307829380035\n",
            "Epoch 91: Loss: 0.2419992834329605\n",
            "Epoch 92: Loss: 0.12653201818466187\n",
            "Epoch 93: Loss: 0.09529101103544235\n",
            "Epoch 94: Loss: 0.04295612871646881\n",
            "Epoch 95: Loss: 0.25153085589408875\n",
            "Epoch 96: Loss: 0.1537177413702011\n",
            "Epoch 97: Loss: 0.01851293444633484\n",
            "Epoch 98: Loss: 0.03994913399219513\n",
            "Epoch 99: Loss: 0.06091621145606041\n",
            "\n",
            "Test set: Average loss: 0.5933, Accuracy: 523/624 (84%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjquAeDh2Sn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose the pneumoniamnist dataset\n",
        "data_flag = 'pneumoniamnist'\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = 2  # Binary classification for pneumoniamnist\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset with transforms\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "train_data = DataClass(split='train', download=True, transform=transform)\n",
        "test_data = DataClass(split='test', download=True, transform=transform)\n",
        "\n",
        "# Split data into 2 parts for clients\n",
        "split_size = len(train_data) // 2\n",
        "client_datasets = [Subset(train_data, range(i * split_size, (i + 1) * split_size)) for i in range(2)]\n",
        "\n",
        "# Create data loaders for each client\n",
        "batch_size = 32\n",
        "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Client Model\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 32, 1568)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hypernetwork\n",
        "class Hypernetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypernetwork, self).__init__()\n",
        "        self.fc = nn.Linear(2 * 1568, 12544)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Server Model\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(12544, 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# Initialize models and move to device\n",
        "clients = [ClientModel().to(device) for _ in range(2)]\n",
        "hypernetwork = Hypernetwork().to(device)\n",
        "server = ServerModel().to(device)\n",
        "\n",
        "# Define optimizers\n",
        "client_optimizers = [optim.SGD(client.parameters(), lr=0.01) for client in clients]\n",
        "hypernetwork_optimizer = optim.SGD(hypernetwork.parameters(), lr=0.01)\n",
        "server_optimizer = optim.SGD(server.parameters(), lr=0.01)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    server.train()\n",
        "    hypernetwork.train()\n",
        "    for client, optimizer in zip(clients, client_optimizers):\n",
        "        client.train()\n",
        "        for data, target in client_loaders[clients.index(client)]:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            client_output = client(data)\n",
        "\n",
        "            # Concatenate client outputs for the hypernetwork\n",
        "            aggregated_output = torch.cat([client_output for client in clients], dim=1)\n",
        "\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            loss = F.nll_loss(server_output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "# def train(epoch):\n",
        "#     server.train()\n",
        "#     hypernetwork.train()\n",
        "#     for client, optimizer in zip(clients, client_optimizers):\n",
        "#         client.train()\n",
        "#         for data, target in client_loaders[clients.index(client)]:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             target = target.squeeze()\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             client_output = client(data)\n",
        "#             hypernetwork_output = hypernetwork(client_output)\n",
        "#             server_output = server(hypernetwork_output)\n",
        "#             loss = F.nll_loss(server_output, target)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    server.eval()\n",
        "    hypernetwork.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            client_outputs = [client(data) for client in clients]\n",
        "            aggregated_output = torch.cat(client_outputs, dim=1)\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            test_loss += F.nll_loss(server_output, target, reduction='sum').item()\n",
        "            pred = server_output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(1, 100):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee102fc1-dd2f-4d35-c4a4-3101957fc104",
        "id": "Vjvle1lhsXiS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist.npz\n",
            "Epoch 1: Loss: 0.6120525002479553\n",
            "Epoch 2: Loss: 0.6775522232055664\n",
            "Epoch 3: Loss: 0.5832805633544922\n",
            "Epoch 4: Loss: 0.37631213665008545\n",
            "Epoch 5: Loss: 0.4235135316848755\n",
            "Epoch 6: Loss: 0.5206459760665894\n",
            "Epoch 7: Loss: 0.7213881015777588\n",
            "Epoch 8: Loss: 0.4024496376514435\n",
            "Epoch 9: Loss: 0.585258960723877\n",
            "Epoch 10: Loss: 0.49598339200019836\n",
            "Epoch 11: Loss: 0.5632988810539246\n",
            "Epoch 12: Loss: 0.503531277179718\n",
            "Epoch 13: Loss: 0.6039111018180847\n",
            "Epoch 14: Loss: 0.5430577993392944\n",
            "Epoch 15: Loss: 0.5195146799087524\n",
            "Epoch 16: Loss: 0.4415673315525055\n",
            "Epoch 17: Loss: 0.489604115486145\n",
            "Epoch 18: Loss: 0.4360673427581787\n",
            "Epoch 19: Loss: 0.5297596454620361\n",
            "Epoch 20: Loss: 0.4747216999530792\n",
            "Epoch 21: Loss: 0.4676307439804077\n",
            "Epoch 22: Loss: 0.31709402799606323\n",
            "Epoch 23: Loss: 0.27337950468063354\n",
            "Epoch 24: Loss: 0.33418604731559753\n",
            "Epoch 25: Loss: 0.2600495219230652\n",
            "Epoch 26: Loss: 0.24138863384723663\n",
            "Epoch 27: Loss: 0.17472952604293823\n",
            "Epoch 28: Loss: 0.21831965446472168\n",
            "Epoch 29: Loss: 0.320265531539917\n",
            "Epoch 30: Loss: 0.33829963207244873\n",
            "Epoch 31: Loss: 0.21865874528884888\n",
            "Epoch 32: Loss: 0.1607600748538971\n",
            "Epoch 33: Loss: 0.22161389887332916\n",
            "Epoch 34: Loss: 0.317730575799942\n",
            "Epoch 35: Loss: 0.244418203830719\n",
            "Epoch 36: Loss: 0.18560640513896942\n",
            "Epoch 37: Loss: 0.10949096083641052\n",
            "Epoch 38: Loss: 0.07781615853309631\n",
            "Epoch 39: Loss: 0.09689861536026001\n",
            "Epoch 40: Loss: 0.2756617069244385\n",
            "Epoch 41: Loss: 0.199880450963974\n",
            "Epoch 42: Loss: 0.17451533675193787\n",
            "Epoch 43: Loss: 0.07044536620378494\n",
            "Epoch 44: Loss: 0.23312507569789886\n",
            "Epoch 45: Loss: 0.2756609320640564\n",
            "Epoch 46: Loss: 0.25836771726608276\n",
            "Epoch 47: Loss: 0.050996530801057816\n",
            "Epoch 48: Loss: 0.13917070627212524\n",
            "Epoch 49: Loss: 0.26029860973358154\n",
            "Epoch 50: Loss: 0.06337085366249084\n",
            "Epoch 51: Loss: 0.2517773509025574\n",
            "Epoch 52: Loss: 0.054881226271390915\n",
            "Epoch 53: Loss: 0.39583370089530945\n",
            "Epoch 54: Loss: 0.11922478675842285\n",
            "Epoch 55: Loss: 0.10956016182899475\n",
            "Epoch 56: Loss: 0.012117831036448479\n",
            "Epoch 57: Loss: 0.1401374638080597\n",
            "Epoch 58: Loss: 0.07378271222114563\n",
            "Epoch 59: Loss: 0.2492273896932602\n",
            "Epoch 60: Loss: 0.18900726735591888\n",
            "Epoch 61: Loss: 0.09398505091667175\n",
            "Epoch 62: Loss: 0.049412164837121964\n",
            "Epoch 63: Loss: 0.20276351273059845\n",
            "Epoch 64: Loss: 0.09649413824081421\n",
            "Epoch 65: Loss: 0.040639787912368774\n",
            "Epoch 66: Loss: 0.16244791448116302\n",
            "Epoch 67: Loss: 0.0751226618885994\n",
            "Epoch 68: Loss: 0.07957164943218231\n",
            "Epoch 69: Loss: 0.04268498718738556\n",
            "Epoch 70: Loss: 0.06541141867637634\n",
            "Epoch 71: Loss: 0.13368871808052063\n",
            "Epoch 72: Loss: 0.06982387602329254\n",
            "Epoch 73: Loss: 0.21334469318389893\n",
            "Epoch 74: Loss: 0.0645345151424408\n",
            "Epoch 75: Loss: 0.12906113266944885\n",
            "Epoch 76: Loss: 0.13772884011268616\n",
            "Epoch 77: Loss: 0.13271716237068176\n",
            "Epoch 78: Loss: 0.18687070906162262\n",
            "Epoch 79: Loss: 0.06648339331150055\n",
            "Epoch 80: Loss: 0.11493229866027832\n",
            "Epoch 81: Loss: 0.09165559709072113\n",
            "Epoch 82: Loss: 0.03303723782300949\n",
            "Epoch 83: Loss: 0.02396232821047306\n",
            "Epoch 84: Loss: 0.04606451839208603\n",
            "Epoch 85: Loss: 0.05217484012246132\n",
            "Epoch 86: Loss: 0.07881471514701843\n",
            "Epoch 87: Loss: 0.023211373016238213\n",
            "Epoch 88: Loss: 0.06384346634149551\n",
            "Epoch 89: Loss: 0.11710158735513687\n",
            "Epoch 90: Loss: 0.10101190209388733\n",
            "Epoch 91: Loss: 0.047282688319683075\n",
            "Epoch 92: Loss: 0.0791267603635788\n",
            "Epoch 93: Loss: 0.07643599063158035\n",
            "Epoch 94: Loss: 0.29662445187568665\n",
            "Epoch 95: Loss: 0.21077875792980194\n",
            "Epoch 96: Loss: 0.09987089037895203\n",
            "Epoch 97: Loss: 0.21480384469032288\n",
            "Epoch 98: Loss: 0.12040063738822937\n",
            "Epoch 99: Loss: 0.1045709028840065\n",
            "\n",
            "Test set: Average loss: 0.4792, Accuracy: 535/624 (86%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}