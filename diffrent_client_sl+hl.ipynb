{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7ls4mQzISkw",
        "outputId": "20074c09-78a1-400a-b306-929acda70463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (9.4.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m564.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.16.0+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->medmnist) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=d06af29a5b84847a6abc4e55114b57261ecc78284d0a94b9621b1f0020f55dde\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, medmnist\n",
            "Successfully installed fire-0.5.0 medmnist-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install medmnist\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose the BreastMNIST dataset\n",
        "data_flag = 'breastmnist'\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = 2  # Binary classification for BreastMNIST\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset with transforms\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "train_data = DataClass(split='train', download=True, transform=transform)\n",
        "test_data = DataClass(split='test', download=True, transform=transform)\n",
        "\n",
        "# Split data into 2 parts for clients\n",
        "split_size = len(train_data) // 2\n",
        "client_datasets = [Subset(train_data, range(i * split_size, (i + 1) * split_size)) for i in range(2)]\n",
        "\n",
        "# Create data loaders for each client\n",
        "batch_size = 32\n",
        "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Client Model\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 32, 1568)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hypernetwork\n",
        "class Hypernetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypernetwork, self).__init__()\n",
        "        self.fc = nn.Linear(2 * 1568, 3136)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Server Model\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(3136, 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# Initialize models and move to device\n",
        "clients = [ClientModel().to(device) for _ in range(2)]\n",
        "hypernetwork = Hypernetwork().to(device)\n",
        "server = ServerModel().to(device)\n",
        "\n",
        "# Define optimizers\n",
        "client_optimizers = [optim.SGD(client.parameters(), lr=0.01) for client in clients]\n",
        "hypernetwork_optimizer = optim.SGD(hypernetwork.parameters(), lr=0.01)\n",
        "server_optimizer = optim.SGD(server.parameters(), lr=0.01)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    server.train()\n",
        "    hypernetwork.train()\n",
        "    for client, optimizer in zip(clients, client_optimizers):\n",
        "        client.train()\n",
        "        for data, target in client_loaders[clients.index(client)]:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            client_output = client(data)\n",
        "\n",
        "            # Concatenate client outputs for the hypernetwork\n",
        "            aggregated_output = torch.cat([client_output for client in clients], dim=1)\n",
        "\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            loss = F.nll_loss(server_output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# def train(epoch):\n",
        "#     server.train()\n",
        "#     hypernetwork.train()\n",
        "#     for client, optimizer in zip(clients, client_optimizers):\n",
        "#         client.train()\n",
        "#         for data, target in client_loaders[clients.index(client)]:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             target = target.squeeze()\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             client_output = client(data)\n",
        "#             hypernetwork_output = hypernetwork(client_output)\n",
        "#             server_output = server(hypernetwork_output)\n",
        "#             loss = F.nll_loss(server_output, target)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    server.eval()\n",
        "    hypernetwork.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            client_outputs = [client(data) for client in clients]\n",
        "            aggregated_output = torch.cat(client_outputs, dim=1)\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            test_loss += F.nll_loss(server_output, target, reduction='sum').item()\n",
        "            pred = server_output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(1, 50):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBvtsvE8pf9H",
        "outputId": "0d25615a-1118-4fc1-f84f-48748383cb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/records/10519652/files/breastmnist.npz?download=1 to /root/.medmnist/breastmnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 559580/559580 [00:00<00:00, 666185.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
            "Epoch 1: Loss: 0.6883583664894104\n",
            "Epoch 2: Loss: 0.671349287033081\n",
            "Epoch 3: Loss: 0.6668608784675598\n",
            "Epoch 4: Loss: 0.6677567958831787\n",
            "Epoch 5: Loss: 0.6654928922653198\n",
            "Epoch 6: Loss: 0.6452265381813049\n",
            "Epoch 7: Loss: 0.6534948945045471\n",
            "Epoch 8: Loss: 0.6489424705505371\n",
            "Epoch 9: Loss: 0.6673902869224548\n",
            "Epoch 10: Loss: 0.7092739939689636\n",
            "Epoch 11: Loss: 0.7181890606880188\n",
            "Epoch 12: Loss: 0.63253253698349\n",
            "Epoch 13: Loss: 0.5538967251777649\n",
            "Epoch 14: Loss: 0.7746447324752808\n",
            "Epoch 15: Loss: 0.5327228903770447\n",
            "Epoch 16: Loss: 0.6791759133338928\n",
            "Epoch 17: Loss: 0.5765900611877441\n",
            "Epoch 18: Loss: 0.5467226505279541\n",
            "Epoch 19: Loss: 0.5927532911300659\n",
            "Epoch 20: Loss: 0.496958464384079\n",
            "Epoch 21: Loss: 0.7370741963386536\n",
            "Epoch 22: Loss: 0.457574725151062\n",
            "Epoch 23: Loss: 0.5658912062644958\n",
            "Epoch 24: Loss: 0.6567019820213318\n",
            "Epoch 25: Loss: 0.5558581352233887\n",
            "Epoch 26: Loss: 0.6098633408546448\n",
            "Epoch 27: Loss: 0.4762454926967621\n",
            "Epoch 28: Loss: 0.6241788864135742\n",
            "Epoch 29: Loss: 0.6209931373596191\n",
            "Epoch 30: Loss: 0.5600357055664062\n",
            "Epoch 31: Loss: 0.7249340415000916\n",
            "Epoch 32: Loss: 0.5043202042579651\n",
            "Epoch 33: Loss: 0.727705180644989\n",
            "Epoch 34: Loss: 0.648365318775177\n",
            "Epoch 35: Loss: 0.5727465748786926\n",
            "Epoch 36: Loss: 0.6074602007865906\n",
            "Epoch 37: Loss: 0.703923761844635\n",
            "Epoch 38: Loss: 0.6786115169525146\n",
            "Epoch 39: Loss: 0.6014495491981506\n",
            "Epoch 40: Loss: 0.6609091758728027\n",
            "Epoch 41: Loss: 0.5213096737861633\n",
            "Epoch 42: Loss: 0.725263237953186\n",
            "Epoch 43: Loss: 0.61922287940979\n",
            "Epoch 44: Loss: 0.7798725366592407\n",
            "Epoch 45: Loss: 0.5593686103820801\n",
            "Epoch 46: Loss: 0.5563123226165771\n",
            "Epoch 47: Loss: 0.5514677166938782\n",
            "Epoch 48: Loss: 0.4939274489879608\n",
            "Epoch 49: Loss: 0.5424672365188599\n",
            "\n",
            "Test set: Average loss: 0.5875, Accuracy: 114/156 (73%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose the BreastMNIST dataset\n",
        "data_flag = 'breastmnist'\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = 2  # Binary classification for BreastMNIST\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset with transforms\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "train_data = DataClass(split='train', download=True, transform=transform)\n",
        "test_data = DataClass(split='test', download=True, transform=transform)\n",
        "\n",
        "# Split data into 2 parts for clients\n",
        "split_size = len(train_data) // 2\n",
        "client_datasets = [Subset(train_data, range(i * split_size, (i + 1) * split_size)) for i in range(2)]\n",
        "\n",
        "# Create data loaders for each client\n",
        "batch_size = 32\n",
        "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Client Model\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 32, 1568)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hypernetwork\n",
        "class Hypernetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypernetwork, self).__init__()\n",
        "        self.fc = nn.Linear(2 * 1568, 6272)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Server Model\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(6272, 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# Initialize models and move to device\n",
        "clients = [ClientModel().to(device) for _ in range(2)]\n",
        "hypernetwork = Hypernetwork().to(device)\n",
        "server = ServerModel().to(device)\n",
        "\n",
        "# Define optimizers\n",
        "client_optimizers = [optim.SGD(client.parameters(), lr=0.01) for client in clients]\n",
        "hypernetwork_optimizer = optim.SGD(hypernetwork.parameters(), lr=0.01)\n",
        "server_optimizer = optim.SGD(server.parameters(), lr=0.01)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    server.train()\n",
        "    hypernetwork.train()\n",
        "    for client, optimizer in zip(clients, client_optimizers):\n",
        "        client.train()\n",
        "        for data, target in client_loaders[clients.index(client)]:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            client_output = client(data)\n",
        "\n",
        "            # Concatenate client outputs for the hypernetwork\n",
        "            aggregated_output = torch.cat([client_output for client in clients], dim=1)\n",
        "\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            loss = F.nll_loss(server_output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# def train(epoch):\n",
        "#     server.train()\n",
        "#     hypernetwork.train()\n",
        "#     for client, optimizer in zip(clients, client_optimizers):\n",
        "#         client.train()\n",
        "#         for data, target in client_loaders[clients.index(client)]:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             target = target.squeeze()\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             client_output = client(data)\n",
        "#             hypernetwork_output = hypernetwork(client_output)\n",
        "#             server_output = server(hypernetwork_output)\n",
        "#             loss = F.nll_loss(server_output, target)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    server.eval()\n",
        "    hypernetwork.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            client_outputs = [client(data) for client in clients]\n",
        "            aggregated_output = torch.cat(client_outputs, dim=1)\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            test_loss += F.nll_loss(server_output, target, reduction='sum').item()\n",
        "            pred = server_output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(1, 100):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8abe286-5cdd-488f-9e53-c8ec67d1bd0d",
        "id": "fSZnAxrRqrBj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
            "Epoch 1: Loss: 0.6768783926963806\n",
            "Epoch 2: Loss: 0.6754679679870605\n",
            "Epoch 3: Loss: 0.6742483377456665\n",
            "Epoch 4: Loss: 0.6575452089309692\n",
            "Epoch 5: Loss: 0.6548781394958496\n",
            "Epoch 6: Loss: 0.66746985912323\n",
            "Epoch 7: Loss: 0.6751784682273865\n",
            "Epoch 8: Loss: 0.6828363537788391\n",
            "Epoch 9: Loss: 0.6424790024757385\n",
            "Epoch 10: Loss: 0.6728195548057556\n",
            "Epoch 11: Loss: 0.6956724524497986\n",
            "Epoch 12: Loss: 0.6534914970397949\n",
            "Epoch 13: Loss: 0.6510999202728271\n",
            "Epoch 14: Loss: 0.6396979093551636\n",
            "Epoch 15: Loss: 0.679373562335968\n",
            "Epoch 16: Loss: 0.6588160991668701\n",
            "Epoch 17: Loss: 0.6643362641334534\n",
            "Epoch 18: Loss: 0.6189576387405396\n",
            "Epoch 19: Loss: 0.6766189336776733\n",
            "Epoch 20: Loss: 0.63462233543396\n",
            "Epoch 21: Loss: 0.65126633644104\n",
            "Epoch 22: Loss: 0.6316800117492676\n",
            "Epoch 23: Loss: 0.6482616066932678\n",
            "Epoch 24: Loss: 0.6430801153182983\n",
            "Epoch 25: Loss: 0.6559993028640747\n",
            "Epoch 26: Loss: 0.6045851707458496\n",
            "Epoch 27: Loss: 0.5920982360839844\n",
            "Epoch 28: Loss: 0.5431761741638184\n",
            "Epoch 29: Loss: 0.4950791001319885\n",
            "Epoch 30: Loss: 0.5898239016532898\n",
            "Epoch 31: Loss: 0.5692979693412781\n",
            "Epoch 32: Loss: 0.4878688156604767\n",
            "Epoch 33: Loss: 0.6444740891456604\n",
            "Epoch 34: Loss: 0.7098135948181152\n",
            "Epoch 35: Loss: 0.574885368347168\n",
            "Epoch 36: Loss: 0.5570607781410217\n",
            "Epoch 37: Loss: 0.594854474067688\n",
            "Epoch 38: Loss: 0.635131299495697\n",
            "Epoch 39: Loss: 0.5721334218978882\n",
            "Epoch 40: Loss: 0.5521530508995056\n",
            "Epoch 41: Loss: 0.5496999621391296\n",
            "Epoch 42: Loss: 0.6036946177482605\n",
            "Epoch 43: Loss: 0.4141928553581238\n",
            "Epoch 44: Loss: 0.6158045530319214\n",
            "Epoch 45: Loss: 0.5120368003845215\n",
            "Epoch 46: Loss: 0.4023725986480713\n",
            "Epoch 47: Loss: 0.7280547618865967\n",
            "Epoch 48: Loss: 0.6037840843200684\n",
            "Epoch 49: Loss: 0.5690816640853882\n",
            "Epoch 50: Loss: 0.6812529563903809\n",
            "Epoch 51: Loss: 0.7689287662506104\n",
            "Epoch 52: Loss: 0.5781567692756653\n",
            "Epoch 53: Loss: 0.5499861836433411\n",
            "Epoch 54: Loss: 0.612205982208252\n",
            "Epoch 55: Loss: 0.5053529143333435\n",
            "Epoch 56: Loss: 0.6298049092292786\n",
            "Epoch 57: Loss: 0.6011084914207458\n",
            "Epoch 58: Loss: 0.7085542678833008\n",
            "Epoch 59: Loss: 0.5645482540130615\n",
            "Epoch 60: Loss: 0.7070387601852417\n",
            "Epoch 61: Loss: 0.7158018946647644\n",
            "Epoch 62: Loss: 0.8215989470481873\n",
            "Epoch 63: Loss: 0.6796466708183289\n",
            "Epoch 64: Loss: 0.6143819689750671\n",
            "Epoch 65: Loss: 0.5594763159751892\n",
            "Epoch 66: Loss: 0.6027953028678894\n",
            "Epoch 67: Loss: 0.7125020623207092\n",
            "Epoch 68: Loss: 0.4721123278141022\n",
            "Epoch 69: Loss: 0.5049839615821838\n",
            "Epoch 70: Loss: 0.5060123801231384\n",
            "Epoch 71: Loss: 0.5933264493942261\n",
            "Epoch 72: Loss: 0.6623622179031372\n",
            "Epoch 73: Loss: 0.4028301239013672\n",
            "Epoch 74: Loss: 0.6294810771942139\n",
            "Epoch 75: Loss: 0.5389743447303772\n",
            "Epoch 76: Loss: 0.721496045589447\n",
            "Epoch 77: Loss: 0.6649694442749023\n",
            "Epoch 78: Loss: 0.5982441306114197\n",
            "Epoch 79: Loss: 0.6023502349853516\n",
            "Epoch 80: Loss: 0.8184540271759033\n",
            "Epoch 81: Loss: 0.5366692543029785\n",
            "Epoch 82: Loss: 0.6057647466659546\n",
            "Epoch 83: Loss: 0.5413930416107178\n",
            "Epoch 84: Loss: 0.6153114438056946\n",
            "Epoch 85: Loss: 0.5580319762229919\n",
            "Epoch 86: Loss: 0.5108516812324524\n",
            "Epoch 87: Loss: 0.6075218915939331\n",
            "Epoch 88: Loss: 0.5519364476203918\n",
            "Epoch 89: Loss: 0.6508702635765076\n",
            "Epoch 90: Loss: 0.6000829339027405\n",
            "Epoch 91: Loss: 0.585665762424469\n",
            "Epoch 92: Loss: 0.49311771988868713\n",
            "Epoch 93: Loss: 0.6984198093414307\n",
            "Epoch 94: Loss: 0.6772884130477905\n",
            "Epoch 95: Loss: 0.6283952593803406\n",
            "Epoch 96: Loss: 0.7904105186462402\n",
            "Epoch 97: Loss: 0.6700550317764282\n",
            "Epoch 98: Loss: 0.4506928622722626\n",
            "Epoch 99: Loss: 0.48833173513412476\n",
            "\n",
            "Test set: Average loss: 0.5793, Accuracy: 114/156 (73%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3casqfOWsCX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose the BreastMNIST dataset\n",
        "data_flag = 'breastmnist'\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = 2  # Binary classification for BreastMNIST\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset with transforms\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "train_data = DataClass(split='train', download=True, transform=transform)\n",
        "test_data = DataClass(split='test', download=True, transform=transform)\n",
        "\n",
        "# Split data into 2 parts for clients\n",
        "split_size = len(train_data) // 2\n",
        "client_datasets = [Subset(train_data, range(i * split_size, (i + 1) * split_size)) for i in range(2)]\n",
        "\n",
        "# Create data loaders for each client\n",
        "batch_size = 32\n",
        "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Client Model\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 32, 1568)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hypernetwork\n",
        "class Hypernetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypernetwork, self).__init__()\n",
        "        self.fc = nn.Linear(2 * 1568, 9408)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Server Model\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(9408,128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# Initialize models and move to device\n",
        "clients = [ClientModel().to(device) for _ in range(2)]\n",
        "hypernetwork = Hypernetwork().to(device)\n",
        "server = ServerModel().to(device)\n",
        "\n",
        "# Define optimizers\n",
        "client_optimizers = [optim.SGD(client.parameters(), lr=0.01) for client in clients]\n",
        "hypernetwork_optimizer = optim.SGD(hypernetwork.parameters(), lr=0.01)\n",
        "server_optimizer = optim.SGD(server.parameters(), lr=0.01)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    server.train()\n",
        "    hypernetwork.train()\n",
        "    for client, optimizer in zip(clients, client_optimizers):\n",
        "        client.train()\n",
        "        for data, target in client_loaders[clients.index(client)]:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            client_output = client(data)\n",
        "\n",
        "            # Concatenate client outputs for the hypernetwork\n",
        "            aggregated_output = torch.cat([client_output for client in clients], dim=1)\n",
        "\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            loss = F.nll_loss(server_output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# def train(epoch):\n",
        "#     server.train()\n",
        "#     hypernetwork.train()\n",
        "#     for client, optimizer in zip(clients, client_optimizers):\n",
        "#         client.train()\n",
        "#         for data, target in client_loaders[clients.index(client)]:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             target = target.squeeze()\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             client_output = client(data)\n",
        "#             hypernetwork_output = hypernetwork(client_output)\n",
        "#             server_output = server(hypernetwork_output)\n",
        "#             loss = F.nll_loss(server_output, target)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    server.eval()\n",
        "    hypernetwork.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            client_outputs = [client(data) for client in clients]\n",
        "            aggregated_output = torch.cat(client_outputs, dim=1)\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            test_loss += F.nll_loss(server_output, target, reduction='sum').item()\n",
        "            pred = server_output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(1, 100):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897d3a30-d380-4672-99f6-2a25409200d3",
        "id": "0hGQdoQFsGjY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
            "Epoch 1: Loss: 0.6930980086326599\n",
            "Epoch 2: Loss: 0.6838165521621704\n",
            "Epoch 3: Loss: 0.6938560605049133\n",
            "Epoch 4: Loss: 0.672053873538971\n",
            "Epoch 5: Loss: 0.6876316666603088\n",
            "Epoch 6: Loss: 0.6692244410514832\n",
            "Epoch 7: Loss: 0.6842911839485168\n",
            "Epoch 8: Loss: 0.678032636642456\n",
            "Epoch 9: Loss: 0.6776852607727051\n",
            "Epoch 10: Loss: 0.642151415348053\n",
            "Epoch 11: Loss: 0.6593918204307556\n",
            "Epoch 12: Loss: 0.6857162117958069\n",
            "Epoch 13: Loss: 0.6832902431488037\n",
            "Epoch 14: Loss: 0.6568840742111206\n",
            "Epoch 15: Loss: 0.6704747676849365\n",
            "Epoch 16: Loss: 0.7113478779792786\n",
            "Epoch 17: Loss: 0.6652343273162842\n",
            "Epoch 18: Loss: 0.6235612034797668\n",
            "Epoch 19: Loss: 0.6905626654624939\n",
            "Epoch 20: Loss: 0.6620615124702454\n",
            "Epoch 21: Loss: 0.6399678587913513\n",
            "Epoch 22: Loss: 0.6233506202697754\n",
            "Epoch 23: Loss: 0.6755709648132324\n",
            "Epoch 24: Loss: 0.6924368739128113\n",
            "Epoch 25: Loss: 0.5685511231422424\n",
            "Epoch 26: Loss: 0.6965975761413574\n",
            "Epoch 27: Loss: 0.6536242365837097\n",
            "Epoch 28: Loss: 0.6350011229515076\n",
            "Epoch 29: Loss: 0.5452654957771301\n",
            "Epoch 30: Loss: 0.6186962723731995\n",
            "Epoch 31: Loss: 0.6791065335273743\n",
            "Epoch 32: Loss: 0.670312225818634\n",
            "Epoch 33: Loss: 0.595588743686676\n",
            "Epoch 34: Loss: 0.6669098138809204\n",
            "Epoch 35: Loss: 0.5346994400024414\n",
            "Epoch 36: Loss: 0.5255128741264343\n",
            "Epoch 37: Loss: 0.7635678052902222\n",
            "Epoch 38: Loss: 0.6072784662246704\n",
            "Epoch 39: Loss: 0.6097634434700012\n",
            "Epoch 40: Loss: 0.5675894021987915\n",
            "Epoch 41: Loss: 0.5672711133956909\n",
            "Epoch 42: Loss: 0.640097439289093\n",
            "Epoch 43: Loss: 0.6632066965103149\n",
            "Epoch 44: Loss: 0.5247375965118408\n",
            "Epoch 45: Loss: 0.6782398223876953\n",
            "Epoch 46: Loss: 0.5834553241729736\n",
            "Epoch 47: Loss: 0.46624958515167236\n",
            "Epoch 48: Loss: 0.4809982180595398\n",
            "Epoch 49: Loss: 0.6633897423744202\n",
            "Epoch 50: Loss: 0.603502631187439\n",
            "Epoch 51: Loss: 0.4033683240413666\n",
            "Epoch 52: Loss: 0.6701489090919495\n",
            "Epoch 53: Loss: 0.6751886606216431\n",
            "Epoch 54: Loss: 0.5461612343788147\n",
            "Epoch 55: Loss: 0.6630107760429382\n",
            "Epoch 56: Loss: 0.5207843780517578\n",
            "Epoch 57: Loss: 0.665729820728302\n",
            "Epoch 58: Loss: 0.5642743110656738\n",
            "Epoch 59: Loss: 0.6234040856361389\n",
            "Epoch 60: Loss: 0.564809262752533\n",
            "Epoch 61: Loss: 0.6652271151542664\n",
            "Epoch 62: Loss: 0.5468543171882629\n",
            "Epoch 63: Loss: 0.4710876941680908\n",
            "Epoch 64: Loss: 0.6617835760116577\n",
            "Epoch 65: Loss: 0.614906370639801\n",
            "Epoch 66: Loss: 0.5558108687400818\n",
            "Epoch 67: Loss: 0.5173454880714417\n",
            "Epoch 68: Loss: 0.6686535477638245\n",
            "Epoch 69: Loss: 0.6548631191253662\n",
            "Epoch 70: Loss: 0.5196377038955688\n",
            "Epoch 71: Loss: 0.6725859642028809\n",
            "Epoch 72: Loss: 0.45639416575431824\n",
            "Epoch 73: Loss: 0.7678595781326294\n",
            "Epoch 74: Loss: 0.5472847819328308\n",
            "Epoch 75: Loss: 0.5911423563957214\n",
            "Epoch 76: Loss: 0.5095850229263306\n",
            "Epoch 77: Loss: 0.6621233224868774\n",
            "Epoch 78: Loss: 0.7564806938171387\n",
            "Epoch 79: Loss: 0.7807847261428833\n",
            "Epoch 80: Loss: 0.7027770280838013\n",
            "Epoch 81: Loss: 0.4590815603733063\n",
            "Epoch 82: Loss: 0.39291298389434814\n",
            "Epoch 83: Loss: 0.641894519329071\n",
            "Epoch 84: Loss: 0.6609321236610413\n",
            "Epoch 85: Loss: 0.6643848419189453\n",
            "Epoch 86: Loss: 0.7337560057640076\n",
            "Epoch 87: Loss: 0.5661882162094116\n",
            "Epoch 88: Loss: 0.6210846304893494\n",
            "Epoch 89: Loss: 0.444621741771698\n",
            "Epoch 90: Loss: 0.6540258526802063\n",
            "Epoch 91: Loss: 0.545927882194519\n",
            "Epoch 92: Loss: 0.7082930207252502\n",
            "Epoch 93: Loss: 0.46539515256881714\n",
            "Epoch 94: Loss: 0.6167081594467163\n",
            "Epoch 95: Loss: 0.6109630465507507\n",
            "Epoch 96: Loss: 0.46873360872268677\n",
            "Epoch 97: Loss: 0.6592112183570862\n",
            "Epoch 98: Loss: 0.6030066609382629\n",
            "Epoch 99: Loss: 0.6931062936782837\n",
            "\n",
            "Test set: Average loss: 0.5740, Accuracy: 114/156 (73%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Emm76puasW4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose the BreastMNIST dataset\n",
        "data_flag = 'breastmnist'\n",
        "info = INFO[data_flag]\n",
        "n_channels = info['n_channels']\n",
        "n_classes = 2  # Binary classification for BreastMNIST\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset with transforms\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "train_data = DataClass(split='train', download=True, transform=transform)\n",
        "test_data = DataClass(split='test', download=True, transform=transform)\n",
        "\n",
        "# Split data into 2 parts for clients\n",
        "split_size = len(train_data) // 2\n",
        "client_datasets = [Subset(train_data, range(i * split_size, (i + 1) * split_size)) for i in range(2)]\n",
        "\n",
        "# Create data loaders for each client\n",
        "batch_size = 32\n",
        "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Client Model\n",
        "class ClientModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClientModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 32, 1568)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Hypernetwork\n",
        "class Hypernetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypernetwork, self).__init__()\n",
        "        self.fc = nn.Linear(2 * 1568, 12544)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Server Model\n",
        "class ServerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ServerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(12544, 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.log_softmax(self.fc2(x), dim=1)\n",
        "\n",
        "# Initialize models and move to device\n",
        "clients = [ClientModel().to(device) for _ in range(2)]\n",
        "hypernetwork = Hypernetwork().to(device)\n",
        "server = ServerModel().to(device)\n",
        "\n",
        "# Define optimizers\n",
        "client_optimizers = [optim.SGD(client.parameters(), lr=0.01) for client in clients]\n",
        "hypernetwork_optimizer = optim.SGD(hypernetwork.parameters(), lr=0.01)\n",
        "server_optimizer = optim.SGD(server.parameters(), lr=0.01)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    server.train()\n",
        "    hypernetwork.train()\n",
        "    for client, optimizer in zip(clients, client_optimizers):\n",
        "        client.train()\n",
        "        for data, target in client_loaders[clients.index(client)]:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            client_output = client(data)\n",
        "\n",
        "            # Concatenate client outputs for the hypernetwork\n",
        "            aggregated_output = torch.cat([client_output for client in clients], dim=1)\n",
        "\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            loss = F.nll_loss(server_output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "# def train(epoch):\n",
        "#     server.train()\n",
        "#     hypernetwork.train()\n",
        "#     for client, optimizer in zip(clients, client_optimizers):\n",
        "#         client.train()\n",
        "#         for data, target in client_loaders[clients.index(client)]:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             target = target.squeeze()\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             client_output = client(data)\n",
        "#             hypernetwork_output = hypernetwork(client_output)\n",
        "#             server_output = server(hypernetwork_output)\n",
        "#             loss = F.nll_loss(server_output, target)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    server.eval()\n",
        "    hypernetwork.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            target = target.squeeze()\n",
        "\n",
        "            client_outputs = [client(data) for client in clients]\n",
        "            aggregated_output = torch.cat(client_outputs, dim=1)\n",
        "            hypernetwork_output = hypernetwork(aggregated_output)\n",
        "            server_output = server(hypernetwork_output)\n",
        "            test_loss += F.nll_loss(server_output, target, reduction='sum').item()\n",
        "            pred = server_output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(1, 100):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac1f841-6d06-4532-9496-97939b40900c",
        "id": "Vjvle1lhsXiS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
            "Epoch 1: Loss: 0.6754650473594666\n",
            "Epoch 2: Loss: 0.6561296582221985\n",
            "Epoch 3: Loss: 0.6628713011741638\n",
            "Epoch 4: Loss: 0.6412376165390015\n",
            "Epoch 5: Loss: 0.6693846583366394\n",
            "Epoch 6: Loss: 0.6676784157752991\n",
            "Epoch 7: Loss: 0.6293926239013672\n",
            "Epoch 8: Loss: 0.6508394479751587\n",
            "Epoch 9: Loss: 0.6944422721862793\n",
            "Epoch 10: Loss: 0.6154557466506958\n",
            "Epoch 11: Loss: 0.6283389329910278\n",
            "Epoch 12: Loss: 0.6770913004875183\n",
            "Epoch 13: Loss: 0.6030018925666809\n",
            "Epoch 14: Loss: 0.5969135165214539\n",
            "Epoch 15: Loss: 0.6581016778945923\n",
            "Epoch 16: Loss: 0.6512589454650879\n",
            "Epoch 17: Loss: 0.609336256980896\n",
            "Epoch 18: Loss: 0.5751552581787109\n",
            "Epoch 19: Loss: 0.6743370294570923\n",
            "Epoch 20: Loss: 0.5975362658500671\n",
            "Epoch 21: Loss: 0.5665944218635559\n",
            "Epoch 22: Loss: 0.7112487554550171\n",
            "Epoch 23: Loss: 0.6511773467063904\n",
            "Epoch 24: Loss: 0.6499750018119812\n",
            "Epoch 25: Loss: 0.7133002281188965\n",
            "Epoch 26: Loss: 0.6431356072425842\n",
            "Epoch 27: Loss: 0.7263868451118469\n",
            "Epoch 28: Loss: 0.4857228100299835\n",
            "Epoch 29: Loss: 0.564484179019928\n",
            "Epoch 30: Loss: 0.6511431932449341\n",
            "Epoch 31: Loss: 0.7468738555908203\n",
            "Epoch 32: Loss: 0.6129278540611267\n",
            "Epoch 33: Loss: 0.5801196098327637\n",
            "Epoch 34: Loss: 0.5548445582389832\n",
            "Epoch 35: Loss: 0.6644308567047119\n",
            "Epoch 36: Loss: 0.4667181074619293\n",
            "Epoch 37: Loss: 0.6969251036643982\n",
            "Epoch 38: Loss: 0.6956886649131775\n",
            "Epoch 39: Loss: 0.6984477043151855\n",
            "Epoch 40: Loss: 0.5579638481140137\n",
            "Epoch 41: Loss: 0.6038492918014526\n",
            "Epoch 42: Loss: 0.5969513058662415\n",
            "Epoch 43: Loss: 0.561275839805603\n",
            "Epoch 44: Loss: 0.5455686450004578\n",
            "Epoch 45: Loss: 0.6050747036933899\n",
            "Epoch 46: Loss: 0.7004503011703491\n",
            "Epoch 47: Loss: 0.7688003778457642\n",
            "Epoch 48: Loss: 0.5049316883087158\n",
            "Epoch 49: Loss: 0.608902633190155\n",
            "Epoch 50: Loss: 0.5604698657989502\n",
            "Epoch 51: Loss: 0.45467695593833923\n",
            "Epoch 52: Loss: 0.5530374646186829\n",
            "Epoch 53: Loss: 0.7167167663574219\n",
            "Epoch 54: Loss: 0.4480569064617157\n",
            "Epoch 55: Loss: 0.5595163702964783\n",
            "Epoch 56: Loss: 0.7179132699966431\n",
            "Epoch 57: Loss: 0.6006613373756409\n",
            "Epoch 58: Loss: 0.6679683923721313\n",
            "Epoch 59: Loss: 0.6053330302238464\n",
            "Epoch 60: Loss: 0.6582626104354858\n",
            "Epoch 61: Loss: 0.6517829895019531\n",
            "Epoch 62: Loss: 0.5454588532447815\n",
            "Epoch 63: Loss: 0.49342894554138184\n",
            "Epoch 64: Loss: 0.6563848257064819\n",
            "Epoch 65: Loss: 0.6676861047744751\n",
            "Epoch 66: Loss: 0.4955405592918396\n",
            "Epoch 67: Loss: 0.6030974984169006\n",
            "Epoch 68: Loss: 0.6063259840011597\n",
            "Epoch 69: Loss: 0.6003464460372925\n",
            "Epoch 70: Loss: 0.6052738428115845\n",
            "Epoch 71: Loss: 0.6619499921798706\n",
            "Epoch 72: Loss: 0.6519553065299988\n",
            "Epoch 73: Loss: 0.5462481379508972\n",
            "Epoch 74: Loss: 0.5490649938583374\n",
            "Epoch 75: Loss: 0.6516990661621094\n",
            "Epoch 76: Loss: 0.4474056363105774\n",
            "Epoch 77: Loss: 0.4409291446208954\n",
            "Epoch 78: Loss: 0.5489336848258972\n",
            "Epoch 79: Loss: 0.5439146161079407\n",
            "Epoch 80: Loss: 0.6505580544471741\n",
            "Epoch 81: Loss: 0.49780336022377014\n",
            "Epoch 82: Loss: 0.44017213582992554\n",
            "Epoch 83: Loss: 0.38695669174194336\n",
            "Epoch 84: Loss: 0.5445830225944519\n",
            "Epoch 85: Loss: 0.7589762210845947\n",
            "Epoch 86: Loss: 0.7043403387069702\n",
            "Epoch 87: Loss: 0.6005223989486694\n",
            "Epoch 88: Loss: 0.6023236513137817\n",
            "Epoch 89: Loss: 0.4909019470214844\n",
            "Epoch 90: Loss: 0.3881990611553192\n",
            "Epoch 91: Loss: 0.5489950180053711\n",
            "Epoch 92: Loss: 0.7042384147644043\n",
            "Epoch 93: Loss: 0.6520566344261169\n",
            "Epoch 94: Loss: 0.6054115891456604\n",
            "Epoch 95: Loss: 0.6562413573265076\n",
            "Epoch 96: Loss: 0.7166867256164551\n",
            "Epoch 97: Loss: 0.6456639170646667\n",
            "Epoch 98: Loss: 0.6093414425849915\n",
            "Epoch 99: Loss: 0.6038097739219666\n",
            "\n",
            "Test set: Average loss: 0.5780, Accuracy: 114/156 (73%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}